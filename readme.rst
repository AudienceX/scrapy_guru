=============
Intro
=============

There is no good tutorial which can show us different ways to develop spider or use tool which can speed up the spider development. So that is why I created this project and hope this might save you a lot of time when you start to write spider.

This project is aimed to help newbie spider developer quickly learn how to develop spider by providing some example and solution. You can get a lot of skills needed when developing spider after you get through all taskls of this project.

The common drawbacks of scrapy tutorials is that the target website they are crawing might change their html structure after a time, so the code will not work at that time. Which might confuse people. That is why I implemented webapp in this project.

--------------------
Support Platform
--------------------

OSX, Linux, python 2.7+, python 3.4+

Python version will not be a problem for us anymore.

--------------------
Workflow
--------------------

Please click the image for better resolution.

.. image:: http://scrapy-guru.readthedocs.io/en/latest/_images/scrapy_tuto.png
    :height: 600px
    :width: 800 px

--------------------
Project structure
--------------------

Here is the directory structure::

    .
    ├── docs
    │   ├── Makefile
    │   ├── build
    │   └── source
    ├── requirements.txt
    ├── spider_project
    │   ├── release
    │   ├── scrapy.cfg
    │   └── spider_project
    └── webapp
        ├── content
        ├── db.sqlite3
        ├── manage.py
        ├── staticfiles
        ├── templates
        └── webapp

* ``docs`` contains the html documentation of this project and it is generated by ``Sphinx`` (Python doc generator)
* ``webapp`` is a web application developed by ``Django`` (A very popular web framework written in Python), we can see this app as a website which show us product info and product links, and we need to write spider to extract the data from it. The hard part is, the web application use many different tech to protect its data against spider, so we need to find way to hack it. It is like a game, right? 
* ``spider_project`` is a project based on ``Scrapy`` which we can write spider in it to extract data from ``webapp``.


--------------------
First glance
--------------------

So here is an example product detail page, it is rendered by ``webapp`` mentioned above.

.. image:: http://scrapy-guru.readthedocs.io/en/latest/_images/first_glance.png

Now according to the `task link_basic_extract`_ in the doc, we need to extract product title and desc from the product detail page

.. _link_basic_extract: http://scrapy-guru.readthedocs.io/en/latest/tasks/basic_extract.html

Here is part of spider code::

    class Basic_extractSpider(scrapy.Spider):
        taskid = "basic_extract"
        name = taskid
        entry = "content/detail_basic"

        def parse_entry_page(self, response):
            item = SpiderProjectItem()
            item["taskid"] = self.taskid
            data = {}
            title = response.xpath("//div[@class='product-title']/text()").extract()
            desc = response.xpath("//section[@class='container product-info']//li/text()").extract()
            data["title"] = title
            data["desc"] = desc

            item["data"] = data
            yield item

We can run the spider now, the spider will start to crawl from the ``self.entry`` and it will check the data scraped automatically. if the data scraped have some mistake, it will give the detail of the error and help you get the spider work as expect.

-----------------------
Keep going
-----------------------

Read doc of this project for more detail and instruction

http://scrapy-guru.readthedocs.io/en/latest/index.html
