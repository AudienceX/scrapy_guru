=============
Intro
=============

I have been developing spider project based on scrapy for almost 3+ years and have extracted info from so many different websites that need to be processed in different ways.

However, there is no good tutorial which can show us different ways to develop or use tool which can speed up the spider development. So that is why I created this project and hope this might save you a lot of time when you start to write spider.

This project is aimed to help newbie developer quickly learn how to develop spider by providing some example and solution. You can get a lot of skills needed when developing spider after you get through all taskls of this project.

--------------------
Support Platform
--------------------

OSX, Linux, python 2.7+, python 3.4+

--------------------
Workflow
--------------------

Please click the image for better resolution.

.. image:: http://scrapy-guru.readthedocs.io/en/latest/_images/scrapy_tuto.png
    :height: 600px
    :width: 800 px

--------------------
Project structure
--------------------

Here is the directory structure::

    .
    ├── docs
    │   ├── Makefile
    │   ├── build
    │   └── source
    ├── requirements.txt
    ├── spider_project
    │   ├── release
    │   ├── scrapy.cfg
    │   └── spider_project
    └── webapp
        ├── content
        ├── db.sqlite3
        ├── manage.py
        ├── staticfiles
        ├── templates
        └── webapp

* ``docs`` contains the html documentation of this project and it is generated by ``Sphinx`` (Python doc generator)
* ``webapp`` is a web application developed by ``Django`` (A very popular web framework written in Python), we can see this app as a website which show us product info and product links, and we need to write spider to extract the data from it. The hard part is, the web application use many different tech to protect its data against spider, so we need to find way to hack it. It is like a game, right? 
* ``spider_project`` is a project based on ``Scrapy`` which we can write spider in it to extract data from ``webapp``.


--------------------
First glance
--------------------

So here is an example product detail page, it is rendered by ``webapp`` mentioned above.

.. image:: http://127.0.0.1:8080/build/html/_images/first_glance.png

Now according to the task list of the doc, we need to extract product title and desc from the product detail page

Here is part of spider::

    class Basic_extractSpider(scrapy.Spider):
        taskid = "basic_extract"
        name = taskid
        entry = "content/detail_basic"

        def parse_entry_page(self, response):
            item = SpiderProjectItem()
            item["taskid"] = self.taskid
            data = {}
            title = response.xpath("//div[@class='product-title']/text()").extract()
            desc = response.xpath("//section[@class='container product-info']//li/text()").extract()
            data["title"] = title
            data["desc"] = desc

            item["data"] = data
            yield item

And then we run the spider, the spider will start to crawl from the ``entry`` and it will check the data scraped. if the data have some mistake, it will give the detail of the error and help you get the spider work as expect.

-----------------------
Keep going
-----------------------

Read doc of this project for more detail and instruction

http://scrapy-guru.readthedocs.io/en/latest/index.html
